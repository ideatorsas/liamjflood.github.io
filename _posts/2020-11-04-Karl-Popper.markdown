---
layout: post
comments: false
title:  "Karl Popper & The Philosophy Of Machine Learning"
date:   2020-11-04
author: Liam Flood
tags: [machine-learning, digital-transformation]
---

While we love the arts we also understand that new technologies, particularly digital, that are out there need to be incorporated sensitively to more analogue experiences.

To take a fresh look at what is the philosophy behind techniques such as machine learning we dug into Popper’s lectures contained in “Conjectures and Refutations”.

Popper’s core idea is around falsification and how a theory can never be proved entirely true. By observation you can increase the probability that a theory is true but you can never state that any theory is absolutely true because the next observation may prove it false.

![Karl Popper](https://literaryocean.com/wp-content/uploads/2020/06/karl-popper.png)

As part of this argument he looks at a pure induction machine and it is this discussion, written long before machine learning had become possible, that some key concepts are introduced.

Hume criticized the idea of induction. This is the idea that knowledge is gathered through observation.

To sum up this logical criticism of Hume’s psychology of induction we may consider the idea of building an induction machine. Placed in a simplified “world” (for example, one of sequences of coloured counters) such a machine may through repetition “learn”, or even “formulate”, laws of succession which hold in it “world”. If such a machine can be constructed (and I have no doubt that it can) then, it might be argued, my theory must be wrong; for if a machine is capable of performing inductions on the basis of repetition, there can be no logical reasons preventing us from doing the same.

The argument sounds convincing, but it is mistaken. In constructing an induction machine we, the architects of the machine, must decide a priori what constitutes its “world”; what things are to be taken as similar or equal; and what kind of “laws” we wish the machine to be able to “discover” in its “world”. In other words we must build into the machine a framework determining what is relevant or interesting in its world: the machine will have its “inborn” selection principles. The problems of similarity will have been solved for it by its makers who have thus interpreted the “world” for the machine.

This was from a lecture in at Cambridge in 1953 long before computing was able to handle the data volumes that machine learning involves. The argument is that if you gain knowledge via observation then your knowledge is based solely on past events and therefore cannot cope with a future event that is not a repetition of the past.

Not only this but you also bring the “world” of the past with it’s assumptions and biases and try to project them on the future. Machine learning ethics are advanced enough to understand this but the algorithms are not able to handle these problems effectively.

So what does this mean for how machine learning should be implemented? You must be aware of the bias that have been created in the model and you must understand that if the “world” changes then the model becomes quickly out of date and possibly dangerous.
